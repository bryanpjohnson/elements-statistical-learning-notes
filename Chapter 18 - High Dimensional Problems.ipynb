{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18 - High Dimensional Problems:  $p >> N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1 - When p is Much Bigger than N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We discuss prediction problems in which the number of features p is much larger than the number of observations N, often written p ≫ N. \n",
    "* Such problems have become of increasing importance, especially in genomics and other areas of computational biology. \n",
    "* High variance and overfitting are a major concern in this setting -- highly regularized approaches often become the methods of choice. \n",
    "* Analysis of high-dimensional data requires either modification of procedures designed for the N > p scenario, or entirely new procedures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2 - Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The simplest form of regularization assumes that the features are independent within each class.\n",
    "* Features will rarely be independent within a class, when p ≫ N we don’t have enough data to estimate their dependencies. The assumption of independence greatly reduces the number of parameters in the model and often results in an effective and interpretable classifier.\n",
    "* Diagonal-covariance LDA rule for classifying the classes -- equivalent to a nearest centroid classifier after appropriate standardization. It is also a special case of the naive-Bayes classifier. It assumes that the features in each class have independent Gaussian distributions with the same variance.\n",
    "* One drawback of the diagonal LDA classifier is that it uses all of the features, and hence is not convenient for interpretation.\n",
    "* Nearest shrunken centroids (NSC) -- we would like to use diagonal-covariance LDA but regularize in a way that automatically drops out features that are not contributing to the class predictions. We can do this by shrinking the classwise mean toward the overall mean, for each feature separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.3 - Linear Classifiers with Quadratic Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDA (regularized discriminant analysis), regularized multinomial logistic regression, and the support vector machine are more complex methods that try to exploit multivariate information in the data. \n",
    "* These of course, can be subjected to $L_1$ or $L_2$ regularization.\n",
    "* These techniques have already been described in detail in previous sections.\n",
    "\n",
    "**Regularized Discriminant Analysis**\n",
    "* Linear discriminant analysis involves the inversion of a p×p within-covariance matrix. When p ≫ N, this matrix can be huge, has rank at most N < p, and hence is singular.\n",
    "* RDA overcomes the singularity issues by regularizing the within-covariance estimate $\\sigma$.\n",
    "\n",
    "**Logistic Regression with Quadratic Regularization**\n",
    "* We use a symmetric version of the multiclass logistic model and apply a variant of $L_2$ regularizations while fitting the log-likelihood via MLE.\n",
    "\n",
    "**The Support Vector Classifier**\n",
    "* When p > N, it is especially attractive because in general the classes are perfectly separable by a hyperplane unless there are identical feature vectors in different classes. \n",
    "* Without any regularization, the support vector classifier finds the separating hyperplane with the largest margin; that is, the hyperplane yielding the biggest gap between the classes in the training data. \n",
    "* Somewhat surprisingly, when p ≫ N the unregularized support vector classifier often works about as well as the best regularized version. \n",
    "* There are many different methods for generalizing the two-class supportvector classifier to K > 2 classes.\n",
    "\n",
    "**Feature Selection**\n",
    "*  Neither discriminant analysis, logistic regression, nor the support-vector classifier perform feature selection automatically, because all use quadratic regularization. \n",
    "* Ad-hoc methods for feature selection have been proposed, for example, removing features with small coefficients, and refitting the classifier. This is done in a backward stepwise manner, starting with the smallest weights and moving on to larger weights. This is known as recursive feature elimination.\n",
    "* These methods can be modified to fit nonlinear decision boundaries using kernels. With p ≫ N the models are already sufficiently complex and overfitting is always a danger. Yet despite the high dimensionality, radial kernels sometimes deliver superior results in these high dimensional problems. \n",
    "* The radial kernel tends to dampen inner products between points far away from each other, which in turn leads to robustness to outliers. \n",
    "\n",
    "**Computational Shortcuts When p ≫ N**\n",
    "* When p > N, the computations can be carried out in an N-dimensional space, rather than p, via the singular value decomposition. \n",
    "* Here is the geometric intuition: just like two points in three-dimensional space always lie on a line, N points in p-dimensional space lie in an (N − 1)-dimensional affine subspace.\n",
    "* Consider the SVD decomposition $X=UDV^T=RV^T$. Plugging into a ridge estimate we can show that we can simply work with the matrix $R$ instead of $X$, which is actually an NxN matrix vs. a pxN.\n",
    "* These results can be generalized to all models that are linear in the parameters and have quadratic penalties. \n",
    "* Geometrically, we are rotating the features to a coordinate system in which all but the first N coordinates are zero. Such rotations are allowed since the quadratic penalty is invariant under rotations, and linear models are equivariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.4 - Linear Classifiers with L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of the L2 penalties discussed in the previous section, we can use L1 penalties which will give us automatic feature selection.\n",
    "* With the LARS algorithm, one can show that when p > N, the number of non-zero coefficients (after applying L1 regularization) is at most N for all values of λ.\n",
    "* Elastic net can be used to compromise. The $L_2$ term encourages highly correlated features to be averaged, while the $L_1$ term encourages a sparse solution in the coefficients of these averaged features.\n",
    "\n",
    "**The Fused Lasso for Functional Data**\n",
    "* We may have functional features $x_{i}(t)$ that are ordered according to some index variable t.\n",
    "* We can represent $x_{i}(t)$ by their coefficients in a basis of functions in t, such as splines, wavelets or Fourier bases, and then apply a regression using these coefficients as predictors. Equivalently, one can instead represent the coefficients of the original features in these bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.5 - Classification When Features are Unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sometimes it is not obvious how to define a feature vector. \n",
    "* As long as we can fill in an N×N proximity matrix of similarities between pairs of objects in our database, it turns out we can put to use many of the classifiers in our arsenal by interpreting the proximities as inner-products.\n",
    "* There are a number of other classifiers, besides the support vector machine, that can be implemented using only inner-product matrices. This also implies they can be “kernelized” like the SVM.\n",
    "* For example, with nearest centroid,  we can compute the distance of the test point to each of the centroids, and perform nearest centroid classification. \n",
    "* K-means clustering, logistic/multinomial regularized regression, LDA, principal components analysis, and more can also be implemented using only inner-products.\n",
    "* What can we not do with inner-products only?\n",
    "    * We cannot standardize the variable.\n",
    "    * We cannot assess directly the contributions of individual variables. \n",
    "    * We cannot separate the good variables from the noise: all variables get an equal say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.6 - High-Dimensional Regression: Supervised Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Principal components analysis is an effective method for finding linear combinations of features that exhibit large variation in a dataset.\n",
    "* However, what we may seek are linear combinations with both high variance *and* significant correlation with the outcome.\n",
    "* To do this, we restrict attention to features which by themselves have a sizable correlation with the outcome.\n",
    "* Connections can be made between Supervised Principal Components and Latent-Variable Modeling which was discussed in a previous chapter.\n",
    "* Further connections can be made between supervised principal components and partial least squares (PLS) regression.\n",
    "* Thresholded PLS can be viewed as a noisy version of supervised principal components.\n",
    "\n",
    "**Preconditioning**\n",
    "* Supervised principal components can yield lower test errors than competing methods, but it does not always produce a sparse model involving only a small number of features.\n",
    "* Highly correlated features will tend to be chosen together, and there may be great deal of redundancy in the set of selected features.\n",
    "* Pre-conditioning seeks the low test error of supervised principal components along with the sparsity of the lasso.\n",
    "* First, we compute the supervised principal component predictor $\\hat{y}_i$ for each observation in the training set (with the threshold selected by cross-validation). Then we apply the lasso with $\\hat{y}_i$ as the outcome variable, in place of the usual outcome $y_i$. All features are used in the lasso fit, not just those that were retained in the thresholding step in supervised principal components.\n",
    "* The idea is that by first denoising the outcome variable, the lasso should not be as adversely affected by the large number of noise features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.7 - Feature Assessment and the Multiple-Testing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sometimes our goal is to assess the significance of individual features vs. accurately predicting the outcome.\n",
    "* The feature assessment problem moves our focus from prediction to the traditional statistical topic of multiple hypothesis testing.\n",
    "* A two-sample *t* statistic can be used to identify informative features which will compare, in the books example, the mean gene (feature) expression of 2 groups of patients.\n",
    "* With a large enough amount of features (genes) some may be falsely significant just by chance. Assessing the large number of genes is called the *multiple testing problem*.\n",
    "* Mostly this section goes over specific examples related to genetic testing and delves into more traditional hypothesis testing methods from statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
