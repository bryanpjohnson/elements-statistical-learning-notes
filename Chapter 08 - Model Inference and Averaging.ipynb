{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - Model Inference and Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.1 - Introduction\n",
    "* Fitting of models up to this point has been by least squares or minimising cross-entropy for classification. Both of these are instances of maximum likelihood approach.\n",
    "* In this chapter, we go over the maximum likelihood approach as well as the Bayesian method for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.2 - The Bootstrap and Maximum Likelihood Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-parameteric bootstrap -- what we discussed in Chapter 7. New datasets are generated without a specific parametric model. They are \"model free\".\n",
    "* Parametric bootstrap -- simulates new responses by adding Gaussian noise to the predicted values.\n",
    "* Likelihood function -- probability of the observed data under the model specificied by $g_\\theta$, the probability density function:  $L(\\theta; Z) = \\prod_{i=1}^{N} g_{\\theta}(z_i)$.\n",
    "* Maximum likelihood function seeks to maximimize the log of the function $L$ by picking the best $\\theta$ given fixed data, $Z$.\n",
    "* The score function is defined as the partial derivative of the log-likelihood function evaluated at $\\theta$ and $Z$. The information matrix, $I(\\theta)$ is the negative of the second partial derivative.\n",
    "* The expected value of $I(\\theta)$ is the Fisher information.\n",
    "* Computing the maximum likelihood uses standard calculus for maximizing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.3 - Bayesian Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Bayesian approach, we specific a sampling density function $Pr(Z|\\theta)$ for our data given the parameters. \n",
    "* We also specify a prior distribution $Pr(\\theta)$ reflecting our knowledge about $\\theta$ before we see the data.\n",
    "* Next, we compute the posterior distribution $Pr(\\theta|Z) = \\frac{Pr(Z|\\theta)Pr(\\theta)}{\\int Pr(Z|\\theta)Pr(\\theta) d\\theta}$. This represents our knowledge about $\\theta$ after we see the data.\n",
    "* This gives you a probability distribution for what $\\theta$ is,  given our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.4 - Relationship Between the Bootstrap and Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The bootstrap distribution represents an approximate, nonparametric, noninformative, posterior distribution for our parameter $\\theta$.\n",
    "* The advantage of the bootstrap is that we did not have to formally specify a prior, nor sample from the posterior -- it is a 'poor mans' Bayes posterior.\n",
    "* By perturbing the data (parametric bootstrap), the bootstrap approximates the Bayesian effect of perturbing the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.5 - The EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tool for simplifying difficult maximum likelihood problems (e.g. Gaussian mixture).\n",
    "* An expectation–maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.\n",
    "* Latent variables are variables that are not directly observed but are rather inferred.\n",
    "* One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points.\n",
    "* The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS: 28682.72602739726\n",
      "Mixture weight 1: 0.6761650172754295\n",
      "Mixture weight 2: 0.3238349827245701\n"
     ]
    }
   ],
   "source": [
    "# sklearn Gaussian Mixture uses EM algorithm to fit\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "\n",
    "gm = GaussianMixture(n_components=2, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, \n",
    "                     init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, \n",
    "                     random_state=None, warm_start=False, verbose=0, verbose_interval=10)\n",
    "\n",
    "# Train the model using the training sets\n",
    "gm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and compute RSS on test set\n",
    "predictions_test = gm.predict(X_test)\n",
    "rss = np.mean((predictions_test - y_test) ** 2)\n",
    "print('RSS: {}'.format(rss))\n",
    "print('Mixture weight 1: {}'.format(gm.weights_[0]))\n",
    "print('Mixture weight 2: {}'.format(gm.weights_[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.6 - MCMC for Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We often want to draw samples from the posterior Bayesian in order to perform inference on the parameters.\n",
    "* Gibbs sampling, an MCMC (Markov Chain Monte Carlo) procedure, is closely related to the EM algorithm. The main difference is that it samples from the conditional distributions instead of maximizing over them.\n",
    "* Suppose we have random variables $U_1, U_2, \\dots, U_K$ and we wish to draw a sample from their joint distribution. Suppose this is difficult to do but it is easy to simulate from the conditional distributions $Pr(U_j | U_1, U_2, ... U_{j-1}, U_{j+1}, \\dots, U_k)$.  Gibbs sampling alternatively simulates from each of these distributions until the process stabilizes.  Once stabilized, it provides a sample from the desired joint distribution.\n",
    "* The samples are not independent for different $t$.\n",
    "* Gibbs sampling produces a Markov chain whose stationary distribution is the true joint distribution.\n",
    "* Gibbs sampling is just one of a number of recently developed procedures for sampling from posterior distributions. It uses conditional sampling of each parameter given the rest, and is useful when the structure of the problem makes this sampling easy to carry out. Other methods do not require such structure, for example the Metropolis–Hastings algorithm.\n",
    "* As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired (typically by thinning the resulting chain of samples by only taking every nth value, e.g. every 100th value). In addition, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [lambda]\n",
      "Sampling 4 chains, 0 divergences: 100%|██████████| 14000/14000 [00:02<00:00, 5589.16draws/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # data\n",
    "    failure = np.array([0., 1.])\n",
    "    value = np.array([1., 0.])\n",
    "    \n",
    "    # Custom log-likelihood\n",
    "    def logp(failure, value):\n",
    "        return tt.sum(failure * tt.log(lam) - lam * value)\n",
    "    \n",
    "    # Model is an exponential distribution\n",
    "    with pm.Model() as model:\n",
    "        lam = pm.Exponential('lambda', 1.)\n",
    "        pm.DensityDist('x', logp, observed={'failure': failure, 'value': value})\n",
    "    return model\n",
    "\n",
    "def run(n_samples=3000):\n",
    "    model = build_model()\n",
    "    \n",
    "    with model:\n",
    "        trace = pm.sample(n_samples)\n",
    "        \n",
    "    return trace\n",
    "\n",
    "trace = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'lambda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(trace.get_values('lambda'))\n",
    "plt.title('Posterior distribution of lambda')\n",
    "plt.xlabel('lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.7 - Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bagging = bootstrap aggregation.  \n",
    "* Bootstrap is a way of assessing the accuracy of a parameter estimate or a prediction, but can also be used to improve the prediction itself. \n",
    "* In Section 8.4 we investigated the relationship between the bootstrap and Bayes approaches, and found that the bootstrap mean is approximately a posterior average. Bagging further exploits this connection.\n",
    "* The bagged estimate is produced by averaging the original predicted value across all bootstramp samples.\n",
    "* For classification, we can understand the bagging effect in terms of a consensus of independent weak learners.\n",
    "* Random forests improve on bagging by reducing the correlation between the sampled trees.  More independence between trees = better.\n",
    "* When we bag a model, any simple structure in the model is lost. As an example, a bagged tree is no longer a tree. For interpretation of the model this is clearly a drawback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1 \n",
      " MSE: 58.42873437154151, Variance: 21.410614829977032\n",
      "\n",
      "Sample: 2 \n",
      " MSE: 57.914211652173904, Variance: 17.644707574872285\n",
      "\n",
      "Sample: 3 \n",
      " MSE: 57.73812079841897, Variance: 22.463031214751048\n",
      "\n",
      "Sample: 4 \n",
      " MSE: 58.22995380237154, Variance: 26.464593412051432\n",
      "\n",
      "Sample: 5 \n",
      " MSE: 59.82179012648221, Variance: 23.82376567927948\n",
      "\n",
      "Sample: 6 \n",
      " MSE: 57.553727976284584, Variance: 19.735853019934698\n",
      "\n",
      "Sample: 7 \n",
      " MSE: 57.74765652173913, Variance: 19.664247502319988\n",
      "\n",
      "Sample: 8 \n",
      " MSE: 60.537175525691694, Variance: 14.479234291740223\n",
      "\n",
      "Sample: 9 \n",
      " MSE: 58.200603154150194, Variance: 17.47038681880673\n",
      "\n",
      "Sample: 10 \n",
      " MSE: 56.85766235573122, Variance: 25.435749326375976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similar to bootstrap example from Chapter 7\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "df = pandas.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "B = 10\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(B):\n",
    "    X_bootstrap, y_bootstrap = resample(X, y)\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=50, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                          metric='minkowski', metric_params=None, n_jobs=-1)\n",
    "    \n",
    "    knn.fit(X_bootstrap, y_bootstrap)\n",
    "    predictions = knn.predict(X)\n",
    "    e = mean_squared_error(y, predictions)\n",
    "    all_predictions.append(predictions)\n",
    "    \n",
    "    df['bootstrap{}'.format(i)] = predictions\n",
    "    \n",
    "    print('Sample: {} \\n MSE: {}, Variance: {}\\n'.format(\n",
    "        i+1,\n",
    "        e,\n",
    "        predictions.var(),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bootstrap0</th>\n",
       "      <th>bootstrap1</th>\n",
       "      <th>bootstrap2</th>\n",
       "      <th>bootstrap3</th>\n",
       "      <th>bootstrap4</th>\n",
       "      <th>bootstrap5</th>\n",
       "      <th>bootstrap6</th>\n",
       "      <th>bootstrap7</th>\n",
       "      <th>bootstrap8</th>\n",
       "      <th>bootstrap9</th>\n",
       "      <th>bagged_estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.000</td>\n",
       "      <td>23.854</td>\n",
       "      <td>24.032</td>\n",
       "      <td>25.838</td>\n",
       "      <td>23.104</td>\n",
       "      <td>26.328</td>\n",
       "      <td>23.542</td>\n",
       "      <td>24.066</td>\n",
       "      <td>23.326</td>\n",
       "      <td>23.480</td>\n",
       "      <td>24.1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.028</td>\n",
       "      <td>26.970</td>\n",
       "      <td>27.396</td>\n",
       "      <td>28.332</td>\n",
       "      <td>26.544</td>\n",
       "      <td>27.500</td>\n",
       "      <td>27.202</td>\n",
       "      <td>27.090</td>\n",
       "      <td>26.212</td>\n",
       "      <td>29.808</td>\n",
       "      <td>27.4082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.838</td>\n",
       "      <td>25.562</td>\n",
       "      <td>28.922</td>\n",
       "      <td>25.820</td>\n",
       "      <td>24.958</td>\n",
       "      <td>25.708</td>\n",
       "      <td>25.130</td>\n",
       "      <td>26.290</td>\n",
       "      <td>25.618</td>\n",
       "      <td>27.336</td>\n",
       "      <td>26.0182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.208</td>\n",
       "      <td>27.870</td>\n",
       "      <td>30.368</td>\n",
       "      <td>28.858</td>\n",
       "      <td>27.216</td>\n",
       "      <td>27.902</td>\n",
       "      <td>25.810</td>\n",
       "      <td>27.040</td>\n",
       "      <td>27.794</td>\n",
       "      <td>28.464</td>\n",
       "      <td>27.7530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.522</td>\n",
       "      <td>27.998</td>\n",
       "      <td>30.088</td>\n",
       "      <td>28.766</td>\n",
       "      <td>27.256</td>\n",
       "      <td>28.358</td>\n",
       "      <td>25.730</td>\n",
       "      <td>26.998</td>\n",
       "      <td>27.562</td>\n",
       "      <td>28.940</td>\n",
       "      <td>27.8218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>25.858</td>\n",
       "      <td>24.450</td>\n",
       "      <td>23.192</td>\n",
       "      <td>24.448</td>\n",
       "      <td>24.460</td>\n",
       "      <td>25.630</td>\n",
       "      <td>24.870</td>\n",
       "      <td>23.012</td>\n",
       "      <td>24.582</td>\n",
       "      <td>27.308</td>\n",
       "      <td>24.7810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>24.914</td>\n",
       "      <td>23.708</td>\n",
       "      <td>24.502</td>\n",
       "      <td>24.936</td>\n",
       "      <td>26.050</td>\n",
       "      <td>26.576</td>\n",
       "      <td>24.526</td>\n",
       "      <td>24.678</td>\n",
       "      <td>24.546</td>\n",
       "      <td>27.934</td>\n",
       "      <td>25.2370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>25.100</td>\n",
       "      <td>24.910</td>\n",
       "      <td>25.642</td>\n",
       "      <td>25.962</td>\n",
       "      <td>25.798</td>\n",
       "      <td>26.956</td>\n",
       "      <td>24.698</td>\n",
       "      <td>22.834</td>\n",
       "      <td>25.034</td>\n",
       "      <td>27.874</td>\n",
       "      <td>25.4808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>25.100</td>\n",
       "      <td>24.828</td>\n",
       "      <td>25.718</td>\n",
       "      <td>26.058</td>\n",
       "      <td>26.808</td>\n",
       "      <td>26.956</td>\n",
       "      <td>24.698</td>\n",
       "      <td>23.018</td>\n",
       "      <td>25.498</td>\n",
       "      <td>27.924</td>\n",
       "      <td>25.6606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>25.068</td>\n",
       "      <td>24.214</td>\n",
       "      <td>25.438</td>\n",
       "      <td>24.624</td>\n",
       "      <td>25.980</td>\n",
       "      <td>26.970</td>\n",
       "      <td>24.282</td>\n",
       "      <td>24.810</td>\n",
       "      <td>24.450</td>\n",
       "      <td>28.058</td>\n",
       "      <td>25.3894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bootstrap0  bootstrap1  bootstrap2  bootstrap3  bootstrap4  bootstrap5  \\\n",
       "0        24.000      23.854      24.032      25.838      23.104      26.328   \n",
       "1        27.028      26.970      27.396      28.332      26.544      27.500   \n",
       "2        24.838      25.562      28.922      25.820      24.958      25.708   \n",
       "3        26.208      27.870      30.368      28.858      27.216      27.902   \n",
       "4        26.522      27.998      30.088      28.766      27.256      28.358   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "501      25.858      24.450      23.192      24.448      24.460      25.630   \n",
       "502      24.914      23.708      24.502      24.936      26.050      26.576   \n",
       "503      25.100      24.910      25.642      25.962      25.798      26.956   \n",
       "504      25.100      24.828      25.718      26.058      26.808      26.956   \n",
       "505      25.068      24.214      25.438      24.624      25.980      26.970   \n",
       "\n",
       "     bootstrap6  bootstrap7  bootstrap8  bootstrap9  bagged_estimate  \n",
       "0        23.542      24.066      23.326      23.480          24.1570  \n",
       "1        27.202      27.090      26.212      29.808          27.4082  \n",
       "2        25.130      26.290      25.618      27.336          26.0182  \n",
       "3        25.810      27.040      27.794      28.464          27.7530  \n",
       "4        25.730      26.998      27.562      28.940          27.8218  \n",
       "..          ...         ...         ...         ...              ...  \n",
       "501      24.870      23.012      24.582      27.308          24.7810  \n",
       "502      24.526      24.678      24.546      27.934          25.2370  \n",
       "503      24.698      22.834      25.034      27.874          25.4808  \n",
       "504      24.698      23.018      25.498      27.924          25.6606  \n",
       "505      24.282      24.810      24.450      28.058          25.3894  \n",
       "\n",
       "[506 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average all predictions\n",
    "df['bagged_estimate'] = np.array(all_predictions).mean(axis=0)\n",
    "df[[c for c in df.columns if 'bootstrap' in c or c == 'bagged_estimate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.8 - Model Averaging and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Averaging the output of multiple model forms.\n",
    "* Committee methods take a simple unweighted average of the predictions from each model.\n",
    "* Weighting by BIC is another more ambitious option. Other non-equal weighting schemes also exist.\n",
    "* Stacked generalization (stacking) takes into account the complexity of the model when weighting the predictions.\n",
    "* Stacking works by fitting a least squares regression on $y_i$ as a function of the model predictions to determine the weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8.9 - Stochastic Search: Bumping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bumping uses bootstrap sampling to move randomly through model space. This can help a fitting method avoid getting stuck in a local minima.\n",
    "* As in bagging, we draw bootstrap samples and fit a model to each. But rather than average the predictions, we choose the model estimated from a bootstrap sample that best fits the training data. We choose the model that produces the smallest prediction error, averaged over the original training set. \n",
    "* By perturbing the data, bumping tries to move the fitting procedure around to good areas of model space. For example, if a few data points are causing the procedure to find a poor solution, any bootstrap sample that omits those data points should procedure a better solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
