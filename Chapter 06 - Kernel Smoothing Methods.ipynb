{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Kernel Smoothing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit a different, but simple model, separately at each point by using observations close to the target point.\n",
    "* Localization is achieved via a weighting function, or kernel, which assigns a weight to observations near the point based on its distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - One-Dimensional Kernel Smoothers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-nearest neighbor regression is an example of a type of kernel smoother.\n",
    "* With nearest neighbor, the size of the kernel adapts to the local density of points.\n",
    "* Other methods will fix the width of the kernel to determine the # of points to average.\n",
    "* Let $\\lambda$ be the smoothing parameter which determines the width of the local neighborhood, then a large $\\lambda$ implies lower variance (averages more observations) but higher bias (assumes the function is constant within the window).\n",
    "* Observation weights create some problems -- with metric neighborhood methods you can simply multiply them by the kernel weight for the weighted average. But with nearest neighborhood, we might want to insist on a minimum neighborhood weight since each observation no longer should count as 1 neighbor.\n",
    "* Metric methods can be badly biased on the boundaries of the domain because of the asymmetry of the kernel in that region. We can alleviate some of this bias by fitting local linear regressions in the kernel.\n",
    "* To even further reduce the bias, we can fit polynomials in the kernel with higher order than just local linear regression. These methods, however, can be victims of the bias-variance tradeoff. Fitting these methods decrease bias at the cost of increased variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Selecting the Width of the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In k-nearest neighbors $\\lambda = k$.\n",
    "* For Epanechnikov or tri-cube kernel with metric width, $\\lambda$ is the radius of the support region.\n",
    "* For Gaussian kernel, $\\lambda$ is the standard deviation.\n",
    "* If window is narrow, variance will be large and bias will be small.\n",
    "* if window is wide, variance will be smaller but bias will be higher.\n",
    "* Similar arguments apply to the local regression estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - Local Regression in $\\mathbb{R}^p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kernel smoothing and local regression generalize naturally to two or more dimensions.\n",
    "* Boundary effects become a much bigger in higher dimensions, as the fraction of points close to the boundary increases to one as $p \\rightarrow \\infty$. Local polynomial regression can correct this problem.\n",
    "* Local regression becomes less useful in dimensions higher than 2 -- it becomes impossible to maintain locality without extraordinary sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Structured Local Regression Models in $\\mathbb{R}^p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When ratio of dimension to sample size is unfavorable, local regression does not help much. However, we can make some structural assumptions about the model.\n",
    "* One approach is to modify the kernel -- a default spherical kernel gives equal weight. We can standardize each variable to unit standard deviation, or more generally, weight diffrent coordinates to downgrade or omit their importance.\n",
    "* Another approach is to consider the ANOVA decomposition and eliminating some of the higher order terms. *Varying coefficient models* belong to this class. \n",
    "* Variable coefficient example -- If we are measuring aorta thickness w.r.t age, gender, and depth down aorta -- we may model the diameter of the aorta as a linear function of age (a longstanding known effect), but allow the coefficients to vary with gender and depth down the aorta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 - Local Likelihood and Other Models\n",
    "* We can fit a broad class of models locally.  Any method can be made local if it accomodates observation weights.\n",
    "* For example, multi-class logistic regression can be used locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 - Kernel Density Estimation and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kernel density estimation is an unsupervised learning procedure that often precedes kernel regression.\n",
    "* Natural estimate for classification is # of points in class / # points in neighborhood. Parzen estimate smooths this, which can be bumpy -- it adds a bit of noise at each point.\n",
    "* Naive Bayes is appropriate when the feature space is high dimension (making density estimation unnattractive). It assumes the function $f$ can be expressed as a product of independent functions $f_k$ for each dimension of features. While generally not true, it simplifies the estimation.\n",
    "* Naive Bayes allows each the conditional densities (for each dimension) to be estimated using one-dimensional kernel density estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 - Radial Basis Functions and Kernels\n",
    "* Combine the flexibility of local fitting with the flexibility gained by fitting basis expansions.\n",
    "* A radial basis function (RBF) is a real-valued function whose value depends only on the distance from the origin, or alternatively on the distance from some other point c, called a center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From sklearn documentation\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "X_kernel = rbf_feature.fit_transform(X)\n",
    "clf = SGDClassifier()   \n",
    "clf.fit(X_kernel, y)\n",
    "clf.score(X_kernel, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature space shape: (4, 2)\n",
      "Kernel feature space shape: (4, 100)\n",
      "Now feed it into your favorite linear algorithm!\n"
     ]
    }
   ],
   "source": [
    "print('Original feature space shape: {}'.format(X.shape))\n",
    "print('Kernel feature space shape: {}'.format(X_kernel.shape))\n",
    "print('Now feed it into your favorite linear algorithm!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 - Mixture Models for Density Estimation and Classification\n",
    "* Mixture models are a weighted sum of gaussian density functions. Useful for probability density estimation.\n",
    "* Mixture models can be viewed as a kind of kernel method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9 - Computational Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With kernel methods, the fitting is done at evaluation or prediction time.  This can provde computationally infeasible with real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
