{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The central idea is to extract linear combinations of the inputs as derived features, and then model the target as a nonlinear function of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 - Projection Pursuit Regression\n",
    " $f(X) = \\sum_{m=1}^{M} g_m(w_m^T X)$ where X is an input with $p$ components.\n",
    "* The non-linear function $g_m$ is called a Ridge function. It varies only in the direction defined by the linear weights vector $w_m$.\n",
    "* The linear combination is the projection of $X$ onto the unit vector $w_m$. \n",
    "* We seek $w_m$ so that the model fits well (i.e. the pursuit).\n",
    "* This model can approximate in continuous function in $\\mathbb{R}^p$. This class of methods are called universal approximators. Generally the flexibility comes at a cost -- they are very hard to interpret.\n",
    "* A model with $M=1$ is more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While there is a lot of hype around neural networks, they are just non-linear statistical models.\n",
    "* A neural network is a 2 stage regression or classification model.\n",
    "* In the first stage, derived features $Z_m$ are created from linear combinations of the inputs. (via an activation function).\n",
    "* Then, the targets are modelled as a function of the linear combinations of the $Z_m$. (using the output function).\n",
    "* The units in the middle of the network, computing the derived features $Z_m$, are called hidden units since they are not direclty observed.\n",
    "* There can be more than 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 - Fitting Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The unknown parameters in the neural network model are often called \"weights\".\n",
    "* There are weights for determining the input to the activation function as well as the linear combination weights in the hidden layer (for passing into the final output function).\n",
    "* The general way to fit is by gradient descent, which is called back-propagation in this setting.\n",
    "* The gradient descent updates can be implemented with a 2 pass algorithm. In the forward pass, the current weights are fixed and the predicted values are computed. In the backward pass, the errors are computed for one set of weights and then back propagated to get the errors for the other set of weights. Both errors are then used to compute the gradients for the updates.\n",
    "* The back propagation procedure is also called the \"delta rule\".\n",
    "* In the back propagation algorithm, each hidden unit passes and receives information only to and from units that share a connection. Hence it can be implemented efficiently on a parallel architecture computer.\n",
    "* A training epoch refers to one sweep through the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5 - Some Issues in Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training neural networks can be difficult -- the model is generally overparametrized, and the optimization problem is nonconvex and unstable unless certain guidelines are followed.\n",
    "* Usually weights are initialized to be random values near zero (so that the model starts out nearly linear).\n",
    "* Neural networks with too many weights are easy to overfit - an early stopping rule is used to stop before we approach the global minimum.\n",
    "* A regularization penalty can also be added to the loss function.\n",
    "* Standardize inputs to have mean zero and std deviation of 1.\n",
    "* Generally speaking, it is better to have too many hidden units than too few. Typically the number of hidden units is somewhere in the range of 5 to 100, with the number increasing with the number of inputs and number of training cases.\n",
    "* Try a variety of random starting weight configurations, and choose the solution giving the lowest (penalized) error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6 - Example - Simulated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A radial function is the most difficult for a neural net, as it is spherically symmetric and with no preferred directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7 - Example - ZIP Code Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If each of the units in a single 8 Ã— 8 feature map share the same set of nine weights (but have their own bias parameter), this forces the extracted features in different parts of the image to be computed by the same linear functional, and consequently these networks are sometimes known as convolutional networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.8 - Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks and PPR are especially effective in problems with a high signal-to-noise ratio and settings where prediction without interpretation is the goal. \n",
    "* They are less effective for problems where the goal is to describe the physical process that generated the data and the roles of individual inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.9 - Bayesian Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Certain modifications to neural nets are possible, such as boosting, bagging, and Bayesian fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 - Computation Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With N observations, p predictors, M hidden units and L training epochs, a neural network fit typically requires $O(NpML)$ operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "379/379 [==============================] - 0s 363us/step - loss: 331.0701\n",
      "Epoch 2/50\n",
      "379/379 [==============================] - 0s 148us/step - loss: 104.3707\n",
      "Epoch 3/50\n",
      "379/379 [==============================] - 0s 125us/step - loss: 85.6194\n",
      "Epoch 4/50\n",
      "379/379 [==============================] - 0s 115us/step - loss: 72.0259\n",
      "Epoch 5/50\n",
      "379/379 [==============================] - 0s 132us/step - loss: 64.2831\n",
      "Epoch 6/50\n",
      "379/379 [==============================] - 0s 130us/step - loss: 60.5505\n",
      "Epoch 7/50\n",
      "379/379 [==============================] - 0s 114us/step - loss: 60.0849\n",
      "Epoch 8/50\n",
      "379/379 [==============================] - 0s 124us/step - loss: 57.9331\n",
      "Epoch 9/50\n",
      "379/379 [==============================] - 0s 110us/step - loss: 55.9146\n",
      "Epoch 10/50\n",
      "379/379 [==============================] - 0s 117us/step - loss: 54.6765\n",
      "Epoch 11/50\n",
      "379/379 [==============================] - 0s 115us/step - loss: 53.6320\n",
      "Epoch 12/50\n",
      "379/379 [==============================] - 0s 108us/step - loss: 52.3482\n",
      "Epoch 13/50\n",
      "379/379 [==============================] - 0s 114us/step - loss: 51.2755\n",
      "Epoch 14/50\n",
      "379/379 [==============================] - 0s 112us/step - loss: 50.4149\n",
      "Epoch 15/50\n",
      "379/379 [==============================] - 0s 110us/step - loss: 49.5057\n",
      "Epoch 16/50\n",
      "379/379 [==============================] - 0s 114us/step - loss: 47.9643\n",
      "Epoch 17/50\n",
      "379/379 [==============================] - 0s 133us/step - loss: 47.4737\n",
      "Epoch 18/50\n",
      "379/379 [==============================] - 0s 120us/step - loss: 46.5051\n",
      "Epoch 19/50\n",
      "379/379 [==============================] - 0s 120us/step - loss: 45.0435\n",
      "Epoch 20/50\n",
      "379/379 [==============================] - 0s 121us/step - loss: 44.2043\n",
      "Epoch 21/50\n",
      "379/379 [==============================] - 0s 118us/step - loss: 44.1927\n",
      "Epoch 22/50\n",
      "379/379 [==============================] - 0s 122us/step - loss: 42.7740\n",
      "Epoch 23/50\n",
      "379/379 [==============================] - 0s 124us/step - loss: 42.9682\n",
      "Epoch 24/50\n",
      "379/379 [==============================] - 0s 127us/step - loss: 41.1426\n",
      "Epoch 25/50\n",
      "379/379 [==============================] - 0s 140us/step - loss: 40.2480\n",
      "Epoch 26/50\n",
      "379/379 [==============================] - 0s 146us/step - loss: 38.7960\n",
      "Epoch 27/50\n",
      "379/379 [==============================] - 0s 131us/step - loss: 38.4617\n",
      "Epoch 28/50\n",
      "379/379 [==============================] - 0s 123us/step - loss: 37.8160\n",
      "Epoch 29/50\n",
      "379/379 [==============================] - 0s 123us/step - loss: 36.5265\n",
      "Epoch 30/50\n",
      "379/379 [==============================] - 0s 132us/step - loss: 35.6272\n",
      "Epoch 31/50\n",
      "379/379 [==============================] - 0s 126us/step - loss: 34.6662\n",
      "Epoch 32/50\n",
      "379/379 [==============================] - 0s 146us/step - loss: 34.3738\n",
      "Epoch 33/50\n",
      "379/379 [==============================] - 0s 121us/step - loss: 33.0980\n",
      "Epoch 34/50\n",
      "379/379 [==============================] - 0s 116us/step - loss: 32.1643\n",
      "Epoch 35/50\n",
      "379/379 [==============================] - 0s 114us/step - loss: 33.6408\n",
      "Epoch 36/50\n",
      "379/379 [==============================] - 0s 114us/step - loss: 31.9922\n",
      "Epoch 37/50\n",
      "379/379 [==============================] - 0s 107us/step - loss: 31.5259\n",
      "Epoch 38/50\n",
      "379/379 [==============================] - 0s 116us/step - loss: 31.1877\n",
      "Epoch 39/50\n",
      "379/379 [==============================] - 0s 119us/step - loss: 29.8623\n",
      "Epoch 40/50\n",
      "379/379 [==============================] - 0s 121us/step - loss: 31.9783\n",
      "Epoch 41/50\n",
      "379/379 [==============================] - 0s 126us/step - loss: 30.4522\n",
      "Epoch 42/50\n",
      "379/379 [==============================] - 0s 127us/step - loss: 29.3934\n",
      "Epoch 43/50\n",
      "379/379 [==============================] - 0s 127us/step - loss: 29.6382\n",
      "Epoch 44/50\n",
      "379/379 [==============================] - 0s 124us/step - loss: 29.5571\n",
      "Epoch 45/50\n",
      "379/379 [==============================] - 0s 145us/step - loss: 29.1220\n",
      "Epoch 46/50\n",
      "379/379 [==============================] - 0s 119us/step - loss: 29.0973\n",
      "Epoch 47/50\n",
      "379/379 [==============================] - 0s 135us/step - loss: 28.6130\n",
      "Epoch 48/50\n",
      "379/379 [==============================] - 0s 120us/step - loss: 27.2770\n",
      "Epoch 49/50\n",
      "379/379 [==============================] - 0s 112us/step - loss: 27.0041\n",
      "Epoch 50/50\n",
      "379/379 [==============================] - 0s 133us/step - loss: 27.0722\n",
      "379/379 [==============================] - 0s 101us/step\n",
      "127/127 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load our toy data set\n",
    "data, target = load_boston(True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target)\n",
    " \n",
    "# Define model architecture\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam')\n",
    "    \n",
    "    return model\n",
    " \n",
    "kr = KerasRegressor(build_fn=nn_model, epochs=50, batch_size=5, verbose=1)\n",
    "kr.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = kr.predict(X_train)\n",
    "test_predictions = kr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 25.811425258867953\n",
      "Test MSE: 35.797897180585295\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train MSE: {mean_squared_error(y_train, train_predictions)}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, test_predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
