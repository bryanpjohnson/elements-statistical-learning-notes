{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 - Support Vector Machines and Flexible Discriminants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Support vector machines (SVM) produce nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 - The Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The goal is to find a hyperplane that creates the biggest margin between the training points for class 1 and âˆ’1.\n",
    "* This is a convex optimization problem. With a convex objective and a convex feasible region, there can be only one optimal solution, which is globally optimal. In a non-convex problem, there may be multiple feasible regions and multiple locally optimal points within each region.  It can take time exponential in the number of variables and constraints to determine that a non-convex problem is infeasible, that the objective function is unbounded, or that an optimal solution is the \"global optimum\" across all feasible regions.\n",
    "* Suppose that the classes overlap in feature space. One way to deal with the overlap is to still maximize M (the margin), but allow for some points to be on the wrong side of the margin.\n",
    "* Points well inside their class boundary do not play a big role in shaping the boundary. This seems like an attractive property, and one that differentiates it from linear discriminant analysis.\n",
    "* A quadratic programming solution using Lagrange multipliers can be computed for the optimization problem.\n",
    "* The method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        14\n",
      "  versicolor       1.00      0.94      0.97        17\n",
      "   virginica       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.96      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: Classification on the Iris data set\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = svm.SVC()\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\n",
    "    classification_report(y_test, clf.predict(X_test), target_names=iris.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 - Support Vector Machines and Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can make the procedure more flexible by enlarging the feature space using basis expansions such as polynomials or splines. Generally, linear boundaries in the enlarged space achieve better training-class separation, and translate to nonlinear boundaries in the original space.\n",
    "* SVMs can be fit with many different loss functions, including those adding a regularization penalty term.\n",
    "*  We can represent the optimization problem and its solution in a special way that only involves the input features via inner products. We do this directly for the transformed feature vectors $h(x_i)$. We then see that for particular choices of h, these inner products can be computed very cheaply.\n",
    "* In the early literature on support vectors, there were claims that the kernel property of the support vector machine is unique to it, and allows one to finesse the curse of dimensionality. Neither of these claims is true.\n",
    "* SVMs can be adapted for regression with a quantitative response, in ways that inherit some of the properties of the SVM classifier.\n",
    "* The support vector machine can be extended to multiclass problems, essentially by solving many two-class problems. A classifier is built for each pair of classes, and the final classifier is the one that dominates the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        13\n",
      "  versicolor       0.89      0.89      0.89         9\n",
      "   virginica       0.94      0.94      0.94        16\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.94      0.94      0.94        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: SVC with poly KERNEL\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# We can change the kernel that the svc uses (in sklearn its default radial)\n",
    "clf = svm.SVC(kernel='poly')\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    classification_report(y_test, clf.predict(X_test), target_names=iris.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 - Generalizing Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the virtues of LDA are as follows:\n",
    "* It is a simple prototype classifier. \n",
    "* The decision boundaries created by LDA are linear, leading to decision rules that are simple to describe and implement.\n",
    "* LDA provides natural low-dimensional views of the data.\n",
    "* Often LDA produces the best classification results because of its simplicity and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the simplicity of LDA causes it to fail in a number of situations as well:\n",
    "* Often linear decision boundaries do not adequately separate the classes.\n",
    "* LDA may use too many parameters, which are estimated with high variance, and its performance suffers. In cases such as this we need to restrict or regularize LDA even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible solutions to generalize LDA and fix these problems are:\n",
    "* Recast the LDA problem as a linear regression problem. Many techniques exist for generalizing linear regression to more flexible, nonparametric forms of regression. This in turn leads to more flexible forms of discriminant analysis, which we call FDA.\n",
    "* Fit an LDA model, but penalize its coefficients to be smooth or otherwise coherent in the spatial domain. We call this procedure penalized discriminant analysis or PDA.\n",
    "* Model each class by a mixture of two or more Gaussians with different centroids, but with every component Gaussian, both within and between classes, sharing the same covariance matrix. This allows for more complex decision boundaries, and allows for subspace reduction as in LDA. We call this extension mixture discriminant analysis or MDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5 - Flexible Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Method for performing LDA using linear regression on derived responses. This in turn leads to nonparametric and flexible alternatives to LDA.\n",
    "* Basically, performs a linear discriminant analysis in an enlarged space (via basis expansion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6 - Penalized Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Although FDA is motivated by generalizing optimal scoring, it can also be viewed directly as a form of regularized discriminant analysis.\n",
    "* After enlarging the set of predictors X via a basis expansion h(X), use (penalized) LDA in the enlarged space.\n",
    "* Experiments have shown that the regularization improves the classification performance of LDA on independent test data by a factor of around 25% in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7 - Mixture Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear discriminant analysis can be viewed as a prototype classifier. Each class is represented by its centroid, and we classify to the closest one using an appropriate metric. \n",
    "* In many situations a single prototype is not sufficient to represent inhomogeneous classes, and mixture models are more appropriate.\n",
    "* The classical and natural method for computing the maximum-likelihood estimates for mixture parameterization is the EM algorithm.\n",
    "* In the E-step, the algorithm apportions the unit weight of an observation in class k to the various subclasses assigned to that class. \n",
    "* In the M-step, an observation in class k is used $R_k$ times to estimate the parameters in each of the $R_k$ component densities, with a different weight for each. (assume that each class k is a mixture of $R_k$ densities).\n",
    "* The M-step turns out to be a weighted version of LDA, with $R = \\sum_{k=1}^{K} R_k$ \"classes\". Furthermore, we can use optimal scoring as before to solve the weighted LDA problem, which allows us to use a weighted version of FDA or PDA at this stage.\n",
    "* By using FDA or PDA in the M-step, we can adapt even more to particular situations. For example, we can fit MDA models to digitized analog signals and images, with smoothness constraints built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With N training cases, p predictors, and m support vectors, the support vector machine requires $m^3 + mN + mpN$ operations, assuming m â‰ˆ N. They do not scale well with N, although computational shortcuts are available.\n",
    "* LDA requires $Np^2 + p^3$ operations, as does PDA. The complexity of FDA depends on the regression method used. Many techniques are linear in N, such as additive models and MARS. General splines and kernel-based regression methods will typically require $N^3$ operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
