{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Boosting and Additive Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 - Boosting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The motivation for boosting -- a procedure that combines the outputs of many “weak” classifiers to produce a powerful “committee.\n",
    "* A weak classifier is one whose error rate is only slightly better than random guessing. The purpose of boosting is to sequentially apply the weak classification algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak classifiers. The predictions from all of the weak classifiers are then combined through a weighted majority vote, producing a final prediction.\n",
    "* The data modifications at each boosting step consist of applying weights to each of the training observation (initially everything is weighted equally).\n",
    "*  At step m, those observations that were misclassified by the classifier induced at the previous step have their weights increased, whereas the weights are decreased for those that were classified correctly. Thus as iterations proceed, observations that are difficult to classify correctly receive ever-increasing influence, forcing it to concentrate on those training observations that are missed by previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 - Boosting Fits and Additive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Boosting is a way of fitting an additive expansion in a set of elementary “basis” functions. Here the basis functions are the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 - Forward Stagewise Additive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Forward stagewise modeling approximates the solution by sequentially adding new basis functions to the expansion, without adjusting the parameters and coefficients of those that have already been added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 - Exponential Loss and Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adaboost is equivalent to forward stagewise additive modeling with loss function $exp(-y f(x))$.\n",
    "* For Adaboost, the basis functions are the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 - Why Exponential Loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its equivalence to forward stagewise additive modeling based on exponential loss was only discovered five years after its inception. \n",
    "* The principal attraction of exponential loss in the context of additive modeling is computational; it leads to the simple modular reweighting AdaBoost algorithm. \n",
    "* The additive expansion produced by AdaBoost is estimating one-half the log-odds of P (Y = 1|x). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 - Loss Functions and Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Robustness = resistance to outliers.\n",
    "* **margin** is $yf(x)$ and is similar to a residual when the response for classification is 1/-1. \n",
    "* If margin < 0 we have misclassified, if > 0 we have correctly classified. The goal of the classification algorithm is to produce positive margins as frequently as possible.\n",
    "* Any loss criterion used for classification should penalize negative margins more heavily than positive ones since positive margin observations are already correctly classified.\n",
    "* Misclassification loss gives unit penalty for negative margin values, and no penalty at all for positive ones. \n",
    "* Both the exponential and deviance loss can be viewed as monotone continuous approximations (i.e. differentiable) to misclassification loss. \n",
    "* Exponential criterion concentrates much more influence on observations with large negative margins. \n",
    "* Binomial deviance concentrates relatively less influence on such observations, more evenly spreading the influence among all of the data.\n",
    "* Squared-error loss is not a good surrogate for misclassification error. \n",
    "* Squared-error loss places much more emphasis on observations with large absolute residuals during the fitting process. It is thus far less robust, and its performance severely degrades for long-tailed error distributions and especially for grossly mismeasured y-values (“outliers”). \n",
    "* Other more robust criteria, such as absolute loss, perform much better in these situations (however this can be difficult to optimize mathematically).\n",
    "* Other loss functions (such as Huber) have been created that are much more outlier resistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 - “Off-the-Shelf” Procedures for Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data is usually messy: the inputs tend to be mixtures of quantitative, binary, and categorical variables, the latter often with many levels. There are generally many missing values, complete observations being rare. Distributions of numeric predictor and response variables are often long-tailed and highly skewed.\n",
    "* Requirements of speed, interpretability, and the messy nature of the data sharply limit the usefulness of most learning procedures as off-the-shelf methods for data mining. \n",
    "* An “off-the-shelf” method is one that can be directly applied to the data without requiring a great deal of time - consuming data preprocessing or careful tuning of the learning procedure.\n",
    "* Of all the well-known learning methods, decision trees come closest to meeting the requirements for serving as an off-the-shelf procedure for data mining.\n",
    "* Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely inaccuracy. They seldom provide predictive accuracy comparable to the best that can be achieved with the data at hand. \n",
    "* Boosting decision trees improves their accuracy, often dramatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.8 - Example:  Spam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Boosting outperforms (w.r.t. test error) additive logistic regression, CART, and MARS on the spam example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.9 - Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A formal mathematical definition of trees is given in this section.\n",
    "* Finding the optimal tree is a difficult optimization problem. So we approximate by doing the following:\n",
    "    * The terminal prediction in each region is just the average of observations.\n",
    "    * Determining the region partitioning is done by a top-down greedy algorithm (the tree partitioning algorithm discussed in previous sections)\n",
    "* Boosted tree model is a sum of these trees. Closed form optimization of this is also nearly impossible. For a few special cases the formula simplifies:\n",
    "    * For squared-errored loss each tree predicts the residuals of the cumulative prediction from the prior step.\n",
    "    * For exponential loss we get the Adabost algorithm.\n",
    "    * Using loss criteria such as the absolute error or the Huber loss in place of squared-error loss for regression, and the deviance in place of exponential loss for classification, will serve to robustify boosting trees. Unfortunately, unlike their non-robust counterparts, these robust criteria do not give rise to simple fast boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.10 - Numerical Optimization via Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fortunately, numerical approximation algorithms can be used to fit boosted trees for any differentiable loss function.\n",
    "* Steepest descent uses a step that is a scalar multiple of the negative gradient of the loss function (goes \"downhill\"). This is a greedy strategy that can find local minima instead of global minima.\n",
    "* Forward stagewise boosting is a greedy strategy. This is the common strategy of fitting to the residual of the current model at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.11 - Right Size Tree For Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Historically assumed each tree was a primitive building block. This resulted in early trees that grow much too deep.\n",
    "* Simple strategy to avoid this is to restrict the algorithm so trees are always the same size (possibly allowing the final tree to be bigger).\n",
    "* Given depth $J$, the interaction level of tree-based approximations is limited by the tree size $J$. Namely, no interaction effects of level greater that $J − 1$ are possible.\n",
    "* $J$ should reflect the level of dominant interactions (which is unknown).\n",
    "* It is unlikely that $J > 10$ will be required. Experience so far indicates that $4 ≤ J ≤ 8$ works well in the context of boosting, with results being fairly insensitive to particular choices in this range. One can fine-tune the value for $J$ by trying several different values and choosing the one that produces the lowest risk on a validation sample. However, this seldom provides significant improvement over using $J ≃ 6$.\n",
    "* Also must estimate $M$, the number of boosting rounds. With early stopping, we can monitor the validation error to make sure we stop when overfit begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.12 - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can control the learning rate of the trees as well by weighting its contribution to the sum.\n",
    "* In fact, the best strategy appears to be to set $ν$ (learning rate) to be very small ($ν$ < 0.1) and then choose M by early stopping. \n",
    "* Another tuning parameter is to subsample the training data at each step. This makes the computation faster and can create a more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 69.8949\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's l2: 64.9347\n",
      "[3]\tvalid_0's l2: 60.7169\n",
      "[4]\tvalid_0's l2: 57.0476\n",
      "[5]\tvalid_0's l2: 53.761\n",
      "[6]\tvalid_0's l2: 50.6526\n",
      "[7]\tvalid_0's l2: 47.8761\n",
      "[8]\tvalid_0's l2: 44.8271\n",
      "[9]\tvalid_0's l2: 42.5575\n",
      "[10]\tvalid_0's l2: 40.5298\n",
      "[11]\tvalid_0's l2: 38.6983\n",
      "[12]\tvalid_0's l2: 36.8774\n",
      "[13]\tvalid_0's l2: 35.2458\n",
      "[14]\tvalid_0's l2: 33.6119\n",
      "[15]\tvalid_0's l2: 32.1138\n",
      "[16]\tvalid_0's l2: 31.0234\n",
      "[17]\tvalid_0's l2: 29.9245\n",
      "[18]\tvalid_0's l2: 28.9536\n",
      "[19]\tvalid_0's l2: 28.1513\n",
      "[20]\tvalid_0's l2: 27.0796\n",
      "[21]\tvalid_0's l2: 26.316\n",
      "[22]\tvalid_0's l2: 25.4827\n",
      "[23]\tvalid_0's l2: 24.7447\n",
      "[24]\tvalid_0's l2: 23.9091\n",
      "[25]\tvalid_0's l2: 23.3001\n",
      "[26]\tvalid_0's l2: 22.736\n",
      "[27]\tvalid_0's l2: 22.0815\n",
      "[28]\tvalid_0's l2: 21.6397\n",
      "[29]\tvalid_0's l2: 21.292\n",
      "[30]\tvalid_0's l2: 20.9276\n",
      "[31]\tvalid_0's l2: 20.4857\n",
      "[32]\tvalid_0's l2: 19.9552\n",
      "[33]\tvalid_0's l2: 19.6219\n",
      "[34]\tvalid_0's l2: 19.1699\n",
      "[35]\tvalid_0's l2: 18.8628\n",
      "[36]\tvalid_0's l2: 18.7085\n",
      "[37]\tvalid_0's l2: 18.5719\n",
      "[38]\tvalid_0's l2: 18.4573\n",
      "[39]\tvalid_0's l2: 18.2066\n",
      "[40]\tvalid_0's l2: 18.0552\n",
      "[41]\tvalid_0's l2: 17.9016\n",
      "[42]\tvalid_0's l2: 17.7682\n",
      "[43]\tvalid_0's l2: 17.6993\n",
      "[44]\tvalid_0's l2: 17.55\n",
      "[45]\tvalid_0's l2: 17.3972\n",
      "[46]\tvalid_0's l2: 17.1756\n",
      "[47]\tvalid_0's l2: 16.9255\n",
      "[48]\tvalid_0's l2: 16.7897\n",
      "[49]\tvalid_0's l2: 16.5704\n",
      "[50]\tvalid_0's l2: 16.4731\n",
      "[51]\tvalid_0's l2: 16.3125\n",
      "[52]\tvalid_0's l2: 16.2017\n",
      "[53]\tvalid_0's l2: 16.1093\n",
      "[54]\tvalid_0's l2: 15.9776\n",
      "[55]\tvalid_0's l2: 15.8031\n",
      "[56]\tvalid_0's l2: 15.5928\n",
      "[57]\tvalid_0's l2: 15.335\n",
      "[58]\tvalid_0's l2: 15.1548\n",
      "[59]\tvalid_0's l2: 14.9157\n",
      "[60]\tvalid_0's l2: 14.7506\n",
      "[61]\tvalid_0's l2: 14.6566\n",
      "[62]\tvalid_0's l2: 14.6178\n",
      "[63]\tvalid_0's l2: 14.5868\n",
      "[64]\tvalid_0's l2: 14.5384\n",
      "[65]\tvalid_0's l2: 14.5114\n",
      "[66]\tvalid_0's l2: 14.389\n",
      "[67]\tvalid_0's l2: 14.2605\n",
      "[68]\tvalid_0's l2: 14.1633\n",
      "[69]\tvalid_0's l2: 14.1216\n",
      "[70]\tvalid_0's l2: 14.0314\n",
      "[71]\tvalid_0's l2: 13.9122\n",
      "[72]\tvalid_0's l2: 13.8252\n",
      "[73]\tvalid_0's l2: 13.6901\n",
      "[74]\tvalid_0's l2: 13.5844\n",
      "[75]\tvalid_0's l2: 13.5007\n",
      "[76]\tvalid_0's l2: 13.4621\n",
      "[77]\tvalid_0's l2: 13.4479\n",
      "[78]\tvalid_0's l2: 13.33\n",
      "[79]\tvalid_0's l2: 13.3508\n",
      "[80]\tvalid_0's l2: 13.2822\n",
      "[81]\tvalid_0's l2: 13.2249\n",
      "[82]\tvalid_0's l2: 13.1114\n",
      "[83]\tvalid_0's l2: 13.033\n",
      "[84]\tvalid_0's l2: 12.9369\n",
      "[85]\tvalid_0's l2: 12.9171\n",
      "[86]\tvalid_0's l2: 12.8986\n",
      "[87]\tvalid_0's l2: 12.8748\n",
      "[88]\tvalid_0's l2: 12.8874\n",
      "[89]\tvalid_0's l2: 12.8773\n",
      "[90]\tvalid_0's l2: 12.8635\n",
      "[91]\tvalid_0's l2: 12.8227\n",
      "[92]\tvalid_0's l2: 12.7913\n",
      "[93]\tvalid_0's l2: 12.7205\n",
      "[94]\tvalid_0's l2: 12.636\n",
      "[95]\tvalid_0's l2: 12.5666\n",
      "[96]\tvalid_0's l2: 12.4854\n",
      "[97]\tvalid_0's l2: 12.4425\n",
      "[98]\tvalid_0's l2: 12.4131\n",
      "[99]\tvalid_0's l2: 12.3829\n",
      "[100]\tvalid_0's l2: 12.3467\n",
      "[101]\tvalid_0's l2: 12.3721\n",
      "[102]\tvalid_0's l2: 12.388\n",
      "[103]\tvalid_0's l2: 12.3603\n",
      "[104]\tvalid_0's l2: 12.384\n",
      "[105]\tvalid_0's l2: 12.3903\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's l2: 12.3467\n",
      "The rmse of prediction is: 3.5137822432117334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load our toy data set\n",
    "data, target = load_boston(True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target)\n",
    "\n",
    "# LightGBM requires Dataset objects to be constructed\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "# Hyper-parameters and settings\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Fit the model with early stopping (monitors test set)\n",
    "gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval, num_boost_round=1000, early_stopping_rounds=5)\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.13 - Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While decisions trees are highly interpretable (you can draw the chart), boosted trees are not so much.\n",
    "* Variable importance - one way to do this is to sum over the trees the squared-error improvement contributed by each variable in an internal node split.\n",
    "* Partial-dependence plot - attempt to understand the nature of the dependence of the approximation $f(X)$ on the most relevant variables joint values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance Score</th>\n",
       "      <th>Feature Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "      <td>Column_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178</td>\n",
       "      <td>Column_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134</td>\n",
       "      <td>Column_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>Column_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98</td>\n",
       "      <td>Column_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98</td>\n",
       "      <td>Column_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78</td>\n",
       "      <td>Column_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>Column_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65</td>\n",
       "      <td>Column_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49</td>\n",
       "      <td>Column_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22</td>\n",
       "      <td>Column_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>Column_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>Column_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Importance Score Feature Name\n",
       "0                186     Column_5\n",
       "1                178    Column_12\n",
       "2                134     Column_7\n",
       "3                126     Column_0\n",
       "4                 98     Column_6\n",
       "5                 98     Column_4\n",
       "6                 78    Column_11\n",
       "7                 71    Column_10\n",
       "8                 65     Column_9\n",
       "9                 49     Column_2\n",
       "10                22     Column_8\n",
       "11                 5     Column_3\n",
       "12                 2     Column_1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "%matplotlib inline\n",
    "\n",
    "importances = gbm.feature_importance()\n",
    "df_importance = pandas.DataFrame(sorted(zip(importances, gbm.feature_name()),\n",
    "                                       reverse=True), \n",
    "columns = ['Importance Score', 'Feature Name'])\n",
    "    \n",
    "df_importance.plot.bar(x='Feature Name', y= 'Importance Score')\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.14 - Illustrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The choice of a particular value of M is not critical, as long as it is not too small. This tends to be the case in many applications. The shrinkage strategy tends to eliminate the problem of overfitting, especially for larger data sets.\n",
    "* Because of their ability to model interactions and automatically select variables, as well as robustness to outliers and missing data, GBM models are rapidly gaining popularity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
